{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98e19e92-1d48-4bc7-9592-aa9f812fbbb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| id| name|salary|\n+---+-----+------+\n|  1|Alice| 50000|\n|  2|  Bob| 60000|\n+---+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Create a Spark session with Delta Lake support\n",
    "spark = SparkSession.builder.appName(\"DeltaLakeDemo\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a Delta Table\n",
    "data = [(1, \"Alice\", 50000), (2, \"Bob\", 60000)]\n",
    "columns = [\"id\", \"name\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/employee\")\n",
    "\n",
    "# Read and display the table\n",
    "df_delta = spark.read.format(\"delta\").load(\"/mnt/delta/employee\")\n",
    "df_delta.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba9fd4a3-6de8-49c9-ac63-8c6486518d36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Demonstrate ACID Transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06e050e7-32bd-4279-b6dd-6964b5f87ffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| id| name|salary|\n+---+-----+------+\n|  1|Alice| 65000|\n|  2|  Bob| 75000|\n+---+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/mnt/delta/employee\")\n",
    "df = df.withColumn(\"salary\", df.salary + 5000)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/employee\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ee5ac7a-7644-4e8c-9e6e-e28610f291c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Example 2: Atomic Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5928a4e0-071c-43b3-9e5d-0086af2e475e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n| id|   name|salary|\n+---+-------+------+\n|  3|Charlie| 75000|\n|  1|  Alice| 65000|\n|  2|    Bob| 75000|\n+---+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \"/mnt/delta/employee\")\n",
    "\n",
    "deltaTable.alias(\"old\").merge(\n",
    "    spark.createDataFrame([(3, \"Charlie\", 70000)], [\"id\", \"name\", \"salary\"]).alias(\"new\"),\n",
    "    \"old.id = new.id\"\n",
    ").whenNotMatchedInsert(values={\"id\": \"new.id\", \"name\": \"new.name\", \"salary\": \"new.salary\"}) \\\n",
    ".execute()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e6fff6d-1192-4635-a292-0a9eca72c0f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Time Travel in Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a1c859c-401f-453e-b909-0713fa15c368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| id| name|salary|\n+---+-----+------+\n|  1|Alice| 50000|\n|  2|  Bob| 60000|\n+---+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/mnt/delta/employee\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950d38f3-f543-413f-82fa-2c06ddd22ca8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n| id|   name|salary|\n+---+-------+------+\n|  3|Charlie| 70000|\n|  1|  Alice| 60000|\n|  2|    Bob| 70000|\n+---+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 3).load(\"/mnt/delta/employee\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8848cd30-802f-4f4e-9d2b-71de761873dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+--------------------------+---------+--------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n|version|timestamp          |userId          |userName                  |operation|operationParameters                   |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                 |userMetadata|engineInfo                         |\n+-------+-------------------+----------------+--------------------------+---------+--------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n|3      |2025-03-18 07:08:28|7073335006293246|mayureshspawashe@gmail.com|WRITE    |{mode -> Overwrite, partitionBy -> []}|null|{2255352527937340}|0318-062413-9mea05x8|2          |WriteSerializable|false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 3288}                                                                                                                      |null        |Databricks-Runtime/11.3.x-scala2.12|\n|2      |2025-03-18 07:08:01|7073335006293246|mayureshspawashe@gmail.com|WRITE    |{mode -> Overwrite, partitionBy -> []}|null|{2255352527937340}|0318-062413-9mea05x8|1          |WriteSerializable|false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 3287}                                                                                                                      |null        |Databricks-Runtime/11.3.x-scala2.12|\n|1      |2025-03-18 06:32:40|7073335006293246|mayureshspawashe@gmail.com|UPDATE   |{predicate -> [\"(id#590L = 1)\"]}      |null|{2255352527937340}|0318-062413-9mea05x8|0          |WriteSerializable|false        |{numRemovedFiles -> 1, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 9540, scanTimeMs -> 7360, numAddedFiles -> 1, numUpdatedRows -> 1, rewriteTimeMs -> 2170}|null        |Databricks-Runtime/11.3.x-scala2.12|\n|0      |2025-03-18 06:32:18|7073335006293246|mayureshspawashe@gmail.com|WRITE    |{mode -> Overwrite, partitionBy -> []}|null|{2255352527937340}|0318-062413-9mea05x8|null       |WriteSerializable|false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 3288}                                                                                                                      |null        |Databricks-Runtime/11.3.x-scala2.12|\n+-------+-------------------+----------------+--------------------------+---------+--------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE HISTORY delta.`/tmp/delta_table`\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "928605dc-c49c-4e0c-9a1f-ae35e04fac2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Z-Ordering and Data Compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cea3f15-09ae-457b-8a59-dd36b3e93216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Define Delta table path\n",
    "delta_table_path = \"/tmp/delta_table\"\n",
    "\n",
    "# Read the existing Delta table\n",
    "df = spark.read.format(\"delta\").load(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "064ad490-4986-46d0-a82f-921d480d18a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartition data to optimize file layout\n",
    "df_repartitioned = df.repartition(4)  # Adjust partition count based on data size\n",
    "\n",
    "# Sort the data manually by the Z-Order column (e.g., 'salary')\n",
    "df_sorted = df_repartitioned.sort(\"salary\")\n",
    "\n",
    "# Overwrite the existing Delta table with optimized layout\n",
    "df_sorted.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2dc3c47-275d-45bb-8ce7-607c1147c469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+--------------------------+---------+--------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n|version|timestamp          |userId          |userName                  |operation|operationParameters                   |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                 |userMetadata|engineInfo                         |\n+-------+-------------------+----------------+--------------------------+---------+--------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n|4      |2025-03-18 09:52:29|7073335006293246|mayureshspawashe@gmail.com|WRITE    |{mode -> Overwrite, partitionBy -> []}|null|{2255352527937340}|0318-062413-9mea05x8|3          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 1129}                                                                                                                      |null        |Databricks-Runtime/11.3.x-scala2.12|\n|3      |2025-03-18 07:08:28|7073335006293246|mayureshspawashe@gmail.com|WRITE    |{mode -> Overwrite, partitionBy -> []}|null|{2255352527937340}|0318-062413-9mea05x8|2          |WriteSerializable|false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 3288}                                                                                                                      |null        |Databricks-Runtime/11.3.x-scala2.12|\n|2      |2025-03-18 07:08:01|7073335006293246|mayureshspawashe@gmail.com|WRITE    |{mode -> Overwrite, partitionBy -> []}|null|{2255352527937340}|0318-062413-9mea05x8|1          |WriteSerializable|false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 3287}                                                                                                                      |null        |Databricks-Runtime/11.3.x-scala2.12|\n|1      |2025-03-18 06:32:40|7073335006293246|mayureshspawashe@gmail.com|UPDATE   |{predicate -> [\"(id#590L = 1)\"]}      |null|{2255352527937340}|0318-062413-9mea05x8|0          |WriteSerializable|false        |{numRemovedFiles -> 1, numCopiedRows -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 9540, scanTimeMs -> 7360, numAddedFiles -> 1, numUpdatedRows -> 1, rewriteTimeMs -> 2170}|null        |Databricks-Runtime/11.3.x-scala2.12|\n|0      |2025-03-18 06:32:18|7073335006293246|mayureshspawashe@gmail.com|WRITE    |{mode -> Overwrite, partitionBy -> []}|null|{2255352527937340}|0318-062413-9mea05x8|null       |WriteSerializable|false        |{numFiles -> 3, numOutputRows -> 3, numOutputBytes -> 3288}                                                                                                                      |null        |Databricks-Runtime/11.3.x-scala2.12|\n+-------+-------------------+----------------+--------------------------+---------+--------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_history = spark.sql(\"DESCRIBE HISTORY delta.`/tmp/delta_table`\")\n",
    "df_history.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89079336-7894-495d-bd9d-fc2aec0cc640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##try with csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a518a8c-dc81-4d2b-95cb-207c87080295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Download the CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddefecea-2bf4-4c29-98e0-c072672af57b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File moved to DBFS: dbfs:/tmp/employees.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Define paths\n",
    "url = \"https://gist.githubusercontent.com/kevin336/acbb2271e66c10a5b73aacf82ca82784/raw/e38afe62e088394d61ed30884dd50a6826eee0a8/employees.csv\"\n",
    "local_path = \"/tmp/employees.csv\"  # Temporary local file\n",
    "dbfs_path = \"dbfs:/tmp/employees.csv\"  # DBFS path\n",
    "\n",
    "# Download file to local temp directory\n",
    "response = requests.get(url)\n",
    "with open(local_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Move file to DBFS\n",
    "dbutils.fs.mv(f\"file:{local_path}\", dbfs_path)\n",
    "\n",
    "print(\"File moved to DBFS:\", dbfs_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10d92a57-0314-40c8-ad1a-1a77fcd36c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Read the File from DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a6e4ea1-1c96-4bb8-9f72-22e080f1380f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|  SH_CLERK|  2600|            - |       124|           50|\n|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|  SH_CLERK|  2600|            - |       124|           50|\n|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03|   AD_ASST|  4400|            - |       101|           10|\n|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|    MK_MAN| 13000|            - |       100|           20|\n|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|    MK_REP|  6000|            - |       201|           20|\n|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|07-JUN-02|    HR_REP|  6500|            - |       101|           40|\n|        204|   Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP| 10000|            - |       101|           70|\n|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR| 12008|            - |       101|          110|\n|        206|   William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|  8300|            - |       205|          110|\n|        100|    Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES| 24000|            - |        - |           90|\n+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"EmployeeData\").getOrCreate()\n",
    "\n",
    "# Read CSV File from DBFS\n",
    "df = spark.read.csv(\"dbfs:/tmp/employees.csv\", header=True, inferSchema=True)\n",
    "df.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55a1c68c-b34c-40c9-9fb2-3dec3088718f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Convert CSV to a Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01b50e59-9418-48cb-b758-639549d16ba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Delta Table Created at: dbfs:/tmp/delta_employees\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Define Delta Table Path\n",
    "delta_path = \"dbfs:/tmp/delta_employees\"\n",
    "\n",
    "# Save as Delta Table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "print(\"‚úÖ Delta Table Created at:\", delta_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94de95aa-b058-41db-ae9e-db08857c4f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Enable ACID Transactions (Update & Delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfed834a-eb69-4511-ad3f-7846639f317f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ACID Transactions Applied: Update & Delete\n"
     ]
    }
   ],
   "source": [
    "# Load Delta Table\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "# Update Transaction: Increase Salary by 10% for employees in 'IT_PROG' job\n",
    "delta_table.update(\n",
    "    condition=\"JOB_ID = 'IT_PROG'\",\n",
    "    set={\"SALARY\": \"SALARY * 1.1\"}\n",
    ")\n",
    "\n",
    "#Delete Transaction: Remove employees with no department\n",
    "delta_table.delete(condition=\"DEPARTMENT_ID IS NULL\")\n",
    "\n",
    "print(\"‚úÖ ACID Transactions Applied: Update & Delete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf8b798a-3580-4322-905b-f547a419425d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|  SH_CLERK|  2600|            - |       124|           50|\n|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|  SH_CLERK|  2600|            - |       124|           50|\n|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03|   AD_ASST|  4400|            - |       101|           10|\n|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|    MK_MAN| 13000|            - |       100|           20|\n|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|    MK_REP|  6000|            - |       201|           20|\n|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|07-JUN-02|    HR_REP|  6500|            - |       101|           40|\n|        204|   Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP| 10000|            - |       101|           70|\n|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR| 12008|            - |       101|          110|\n|        206|   William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|  8300|            - |       205|          110|\n|        100|    Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES| 24000|            - |        - |           90|\n|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|21-SEP-05|     AD_VP| 17000|            - |       100|           90|\n|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|13-JAN-01|     AD_VP| 17000|            - |       100|           90|\n|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|03-JAN-06|   IT_PROG|  9900|            - |       102|           60|\n|        104|     Bruce|    Ernst|  BERNST|590.423.4568|21-MAY-07|   IT_PROG|  6600|            - |       103|           60|\n|        105|     David|   Austin| DAUSTIN|590.423.4569|25-JUN-05|   IT_PROG|  5280|            - |       103|           60|\n|        106|     Valli|Pataballa|VPATABAL|590.423.4560|05-FEB-06|   IT_PROG|  5280|            - |       103|           60|\n|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|07-FEB-07|   IT_PROG|  4620|            - |       103|           60|\n|        108|     Nancy|Greenberg|NGREENBE|515.124.4569|17-AUG-02|    FI_MGR| 12008|            - |       101|          100|\n|        109|    Daniel|   Faviet| DFAVIET|515.124.4169|16-AUG-02|FI_ACCOUNT|  9000|            - |       108|          100|\n|        110|      John|     Chen|   JCHEN|515.124.4269|28-SEP-05|FI_ACCOUNT|  8200|            - |       108|          100|\n+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Read the updated Delta table\n",
    "df_updated = spark.read.format(\"delta\").load(delta_path)\n",
    "df_updated.show(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3d4007a-0a93-4d9f-8a7e-93f38fc9af6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Use Time Travel to Query Old Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5244c3e5-cc3f-49a9-bbd5-d5a7ff592b43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|EMAIL   |PHONE_NUMBER|HIRE_DATE|JOB_ID    |SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n|198        |Donald    |OConnell |DOCONNEL|650.507.9833|21-JUN-07|SH_CLERK  |2600  | -            |124       |50           |\n|199        |Douglas   |Grant    |DGRANT  |650.507.9844|13-JAN-08|SH_CLERK  |2600  | -            |124       |50           |\n|200        |Jennifer  |Whalen   |JWHALEN |515.123.4444|17-SEP-03|AD_ASST   |4400  | -            |101       |10           |\n|201        |Michael   |Hartstein|MHARTSTE|515.123.5555|17-FEB-04|MK_MAN    |13000 | -            |100       |20           |\n|202        |Pat       |Fay      |PFAY    |603.123.6666|17-AUG-05|MK_REP    |6000  | -            |201       |20           |\n|203        |Susan     |Mavris   |SMAVRIS |515.123.7777|07-JUN-02|HR_REP    |6500  | -            |101       |40           |\n|204        |Hermann   |Baer     |HBAER   |515.123.8888|07-JUN-02|PR_REP    |10000 | -            |101       |70           |\n|205        |Shelley   |Higgins  |SHIGGINS|515.123.8080|07-JUN-02|AC_MGR    |12008 | -            |101       |110          |\n|206        |William   |Gietz    |WGIETZ  |515.123.8181|07-JUN-02|AC_ACCOUNT|8300  | -            |205       |110          |\n|100        |Steven    |King     |SKING   |515.123.4567|17-JUN-03|AD_PRES   |24000 | -            | -        |90           |\n|101        |Neena     |Kochhar  |NKOCHHAR|515.123.4568|21-SEP-05|AD_VP     |17000 | -            |100       |90           |\n|102        |Lex       |De Haan  |LDEHAAN |515.123.4569|13-JAN-01|AD_VP     |17000 | -            |100       |90           |\n|103        |Alexander |Hunold   |AHUNOLD |590.423.4567|03-JAN-06|IT_PROG   |9900  | -            |102       |60           |\n|104        |Bruce     |Ernst    |BERNST  |590.423.4568|21-MAY-07|IT_PROG   |6600  | -            |103       |60           |\n|105        |David     |Austin   |DAUSTIN |590.423.4569|25-JUN-05|IT_PROG   |5280  | -            |103       |60           |\n|106        |Valli     |Pataballa|VPATABAL|590.423.4560|05-FEB-06|IT_PROG   |5280  | -            |103       |60           |\n|107        |Diana     |Lorentz  |DLORENTZ|590.423.5567|07-FEB-07|IT_PROG   |4620  | -            |103       |60           |\n|108        |Nancy     |Greenberg|NGREENBE|515.124.4569|17-AUG-02|FI_MGR    |12008 | -            |101       |100          |\n|109        |Daniel    |Faviet   |DFAVIET |515.124.4169|16-AUG-02|FI_ACCOUNT|9000  | -            |108       |100          |\n|110        |John      |Chen     |JCHEN   |515.124.4269|28-SEP-05|FI_ACCOUNT|8200  | -            |108       |100          |\n+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\nonly showing top 20 rows\n\n+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|  SH_CLERK|  2600|            - |       124|           50|\n|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|  SH_CLERK|  2600|            - |       124|           50|\n|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03|   AD_ASST|  4400|            - |       101|           10|\n|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|    MK_MAN| 13000|            - |       100|           20|\n|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|    MK_REP|  6000|            - |       201|           20|\n|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|07-JUN-02|    HR_REP|  6500|            - |       101|           40|\n|        204|   Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP| 10000|            - |       101|           70|\n|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR| 12008|            - |       101|          110|\n|        206|   William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|  8300|            - |       205|          110|\n|        100|    Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES| 24000|            - |        - |           90|\n+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Show Delta Table History (Versions)\n",
    "spark.read.format(\"delta\").option(\"history\", True).load(delta_path).show(truncate=False)\n",
    "\n",
    "# Query Previous Version (Before Update)\n",
    "df_old = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_path)\n",
    "df_old.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1c87825-6e2d-40c9-afd0-455924c70dc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Apply Z-Ordering for Faster Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56940261-4db1-4e12-abb6-ef6326358f69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Z-Ordering Applied for Optimization\n"
     ]
    }
   ],
   "source": [
    "#  Perform Z-Ordering on JOB_ID for faster queries\n",
    "spark.sql(f\"OPTIMIZE delta.`{delta_path}` ZORDER BY JOB_ID\")\n",
    "\n",
    "print(\"‚úÖ Z-Ordering Applied for Optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "788478f1-33d5-4ba8-89e7-04fafcd659fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+---------+-------+------+--------------+----------+-------------+\n|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE| JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n+-----------+----------+---------+--------+------------+---------+-------+------+--------------+----------+-------------+\n|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|03-JAN-06|IT_PROG|  9900|            - |       102|           60|\n|        104|     Bruce|    Ernst|  BERNST|590.423.4568|21-MAY-07|IT_PROG|  6600|            - |       103|           60|\n|        105|     David|   Austin| DAUSTIN|590.423.4569|25-JUN-05|IT_PROG|  5280|            - |       103|           60|\n|        106|     Valli|Pataballa|VPATABAL|590.423.4560|05-FEB-06|IT_PROG|  5280|            - |       103|           60|\n|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|07-FEB-07|IT_PROG|  4620|            - |       103|           60|\n+-----------+----------+---------+--------+------------+---------+-------+------+--------------+----------+-------------+\n\n‚è≥ Query Time (Before Optimization): 1.062964677810669 seconds\n+-----------+----------+---------+--------+------------+---------+-------+------+--------------+----------+-------------+\n|EMPLOYEE_ID|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE| JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n+-----------+----------+---------+--------+------------+---------+-------+------+--------------+----------+-------------+\n|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|03-JAN-06|IT_PROG|  9900|            - |       102|           60|\n|        104|     Bruce|    Ernst|  BERNST|590.423.4568|21-MAY-07|IT_PROG|  6600|            - |       103|           60|\n|        105|     David|   Austin| DAUSTIN|590.423.4569|25-JUN-05|IT_PROG|  5280|            - |       103|           60|\n|        106|     Valli|Pataballa|VPATABAL|590.423.4560|05-FEB-06|IT_PROG|  5280|            - |       103|           60|\n|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|07-FEB-07|IT_PROG|  4620|            - |       103|           60|\n+-----------+----------+---------+--------+------------+---------+-------+------+--------------+----------+-------------+\n\nüöÄ Query Time (After Optimization): 0.8520703315734863 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Run query before optimization\n",
    "start_time = time.time()\n",
    "spark.read.format(\"delta\").load(delta_path).filter(\"JOB_ID = 'IT_PROG'\").show()\n",
    "print(\"‚è≥ Query Time (Before Optimization):\", time.time() - start_time, \"seconds\")\n",
    "\n",
    "# Run query after optimization\n",
    "start_time = time.time()\n",
    "spark.sql(f\"SELECT * FROM delta.`{delta_path}` WHERE JOB_ID = 'IT_PROG'\").show()\n",
    "print(\"üöÄ Query Time (After Optimization):\", time.time() - start_time, \"seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2208891892777440,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "ad_DB_Day1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
